# Copyright (C) 4/3/21 RW Bunney

# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.

# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.

# You should have received a copy of the GNU General Public License
# along with this program.  If not, see <https://www.gnu.org/licenses/>.

"""
The Pipelines suggested in the SDP parametric model have components. These
pipeline components are what makes up our different pipelines (e.g. continuum
imaging pipeline, spectral etc.). The parametric model provides us with a
calculation of the compute associated with these models.
"""
import json
import math
import pandas as pd

WORKFLOW = "eagle/sdp_continuum_tests.json"
PIPELINE_DATA_CSV = "csv/SKA1_Low_hpso_pandas.csv"

PIPELINE_WORKFLOW = {
    "dprepa": '"eagle/SDPContinuumPipelineNoOuter_Maj10-Min10.graph"'
}


class SI:
    kilo = 10 ** 3
    mega = 10 ** 6
    giga = 10 ** 9
    tera = 10 ** 12
    peta = 10 ** 15


def generate_workflow_from_observation(observation):
    """
    Given a pipeline and observation specification, generate a workflow file
    and return the path name
    Parameters
    ----------
    observation : dict
        Dictionary of observation data, including duration

    Returns
    -------
            'type':pipeline,

    """
    return None

def generate_cost_per_product(workflow, product_table, hpso, pipeline):
    """
    Produce a cost value per node within the workflow graph for the given
    product.

    For a given workflow, there will be a product from the HPSO (e.g. Grid,
    Subtract Image etc.) which, based on the SDP Parametric Model,
    has a value which is the total expected PFLOP/s expected for that
    component over the lifetime of the workflow.

    As per sdp-par-model.parameters.equations., we know
    the PFlop/s is generated by dividing by o.Tobs (the observation time in
    seconds). From this we can back-calculate total FLOPS/product for the
    entire workflow, and then divide this based on the number of
    product-tasks we have within the workflow.

    Parameters
    ----------
    workflow : dictionary of the JSON graph we are focusing on

    product_table : pd.DataFrame
        Pandas dataframe containing the components

    hpso : str
        the HPSO we are generating.

    cluster: str
        path to the cluster specification required of the observation.


    Returns
    -------

    """

    ignore_components = [
        'UpdateGSM', 'UpdateLSM', 'FinishMinorCycle', 'BeginMinorCycle'
    ]

    total_product_costs = {}
    for element in workflow:
        # Name of task in DALiuGE workflow is 'nm'
        if 'outputs' in element.keys():
            name = element['nm']
            if name not in total_product_costs:
                if name in ignore_components:
                    # These are not 'products' that take compute time
                    total_product_costs[name] = 0
                else:
                    df = product_table[['Pipeline', 'hpso', name]]
                    df = df[(df['Pipeline'] == pipeline) & (df['hpso'] == hpso)]
                    value = float(df[name])
                    total_product_costs[name] = value

    return total_product_costs


def assign_costs_to_workflow(workflow, costs, observation, system_sizing):
    """
    For a given set of costs, calculate the amount per-task in the workflow
    is necessary.
    Parameters
    ----------
    workflow : dictionary
        Dictionary representation of JSON object that is converted from EAGLE
        LGT through DALiuGE
    costs : dict
        product-compute costs (petaflops/s) pairs for each component in the
        workflow
    observation : dict
        This is a list of requirements associated with the observation,
        which we use to determine the amount of compute associated with it
        e.g. length (max 5 hours), number of baselines (max 512) etc.

    Notes
    -----
    The idea is that for a given component (e.g. Grid) there is a set compute

    Returns
    -------

    """
    pipelines = {
        "pipelines": {}
    }

    # generate pipeline total ingest costs:
    max_ingest = system_sizing[
        system_sizing['HPSO'] == 'hpso01'
        ]['Total Compute Requirement [PetaFLOP/s]']
    observation_ingest = tel_pecentage * (float(max_ingest) * SI.peta)
    tel_pecentage = channels / float(MAX_CHANNELS)

    ingest_cluster_demand = _find_ingest_demand(cluster, observation_ingest)

    final_workflow = []
    ecounter = {}


    # count prevalence of each component in the workflow
    for element in workflow:
        if 'outputs' in element.keys():
            if element['nm'] in ecounter:
                ecounter[element['nm']] += 1
            else:
                ecounter[element['nm']] = 1

    for element in workflow:
        if 'outputs' in element.keys():
            name = element['nm']
            if name in ecounter:
                if costs[name] == -1:
                    continue
                else:
                    individual_cost = costs[name] / ecounter[name] * SI.peta
                    element['tw'] = individual_cost
                    final_workflow.append(element)
        else:
            final_workflow.append(element)

    return final_workflow


def _find_ingest_demand(cluster, ingest_flops):
    """
    Get the average compute over teh CPUs in the cluster and determine the
    number of resources necessary for the current ingest_flops

    "cluster": {
        "header": {
            "time": "false",
            "generator": "hpconfig",
            "architecture": {
                "cpu": {
                    "XeonIvyBridge": 50,
                    "XeonSandyBridge": 100
                },
                "gpu": {
                    "NvidiaKepler": 64
                }
            }
        },

    """
    arch = cluster['cluster']['header']['architecture']
    cpus = arch['cpu'].keys()
    m = 0

    for cpu in cpus:
        m += cluster['cluster']['system']['resources'][f'{cpu}_m0'][
            'flops']
    sys_average = m / len(cpus)
    num_machines = math.ceil(ingest_flops / sys_average)
    return num_machines


if __name__ == '__main__':
    df_pipeline = pd.read_csv(PIPELINE_DATA_CSV)
    with open(WORKFLOW) as f:
        workflow = json.load(f)
    hpso = 'hpso01'
    pipeline = 'DPrepA'
    costs = generate_cost_per_product(workflow, df_pipeline, hpso, pipeline)
    graph = assign_costs_to_workflow(workflow, costs, None, None)
